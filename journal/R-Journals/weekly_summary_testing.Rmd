---
title: "Weekly Reporting"
author: "agrotech live | wigglelabs"
date: "`r Sys.Date()`"
output: pdf_document
header-includes:
  - \usepackage{titling}
  - \pretitle{\begin{flushleft}\Huge}  # Title formatting
  - \preauthor{\begin{flushleft}\large}  # Author formatting
  - \predate{\begin{flushleft}\large}  # Date formatting
  - \posttitle{\end{flushleft}}  # End title format
  - \postauthor{\end{flushleft}}  # End author format
  - \postdate{\end{flushleft}}    # End date format
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Abstract
In an ongoing effort to achieve food sovereignty, Wiggle Labs has developed Agrotech Live to monitor the soil health of different plants and crops. This tool collects four data points; Temperature, Moisture, Light and Conductivity from sensors placed near a subject. The training data for this program are the ideal conditions for the subject, and the performance of the experiment is based on how close the collected sensor (testing) data is to the input care training data. Input data is identical in structure the testing data (collected during a session), except it represents only perfect conditions for the subject. A score is generated (along with other statistical results) periodically to communicate how well the experiment is performing.

This report covers the past seven days of session data. If there are missing records, then the closest recorded days will be included.

# Stage 1: Time Series, Correlation, Covariance and Heatmap

## Time Series Comparison

This section examines the relationships between sensor variables from the past week and the initial input data we're comparing the session data to. 



```{r preprocess-data, include=FALSE}
suppressPackageStartupMessages(library(dplyr))

# Load data
session_data <- read.csv("data/session.csv")
training_data <- read.csv("data/training.csv")

# Resizing training data if needed
min_len <- min(nrow(training_data), nrow(session_data))
training_data <- training_data[1:min_len, ]
session_data  <- session_data[1:min_len, ]

# Convert timestamps (if needed for other use)
session_data$Timestamp <- as.POSIXct(session_data$Timestamp, format = "%Y-%m-%d %H:%M:%S")
training_data$Timestamp <- as.POSIXct(training_data$Timestamp, format = "%Y-%m-%d %H:%M:%S")


```

```{r plot-sensor-data, echo=FALSE, fig.width=10, fig.height=8}
# Sort both datasets by time to ensure chronological order
training_data <- training_data[order(training_data$Timestamp), ]
session_data  <- session_data[order(session_data$Timestamp), ]

# Align by earliest common start date
start_date <- min(min(training_data$Timestamp), min(session_data$Timestamp))
training_data <- training_data[training_data$Timestamp >= start_date, ]
session_data  <- session_data[session_data$Timestamp >= start_date, ]

# Colors
train_col <- "green"
sess_col  <- "#5B9BD5"

# Plotting function
plot_overlay <- function(train_y, train_time, sess_y, sess_time, ylabel, title) {
  len <- min(length(train_y), length(sess_y))
  plot(train_time[1:len], train_y[1:len],
       type = "l", col = train_col, lwd = 2,
       xlab = "Timestamp", ylab = ylabel, main = title)
  lines(sess_time[1:len], sess_y[1:len], col = sess_col, lwd = 2, lty = 2)
  legend("topright", legend = c("Training", "Session"), col = c(train_col, sess_col),
         lty = c(1, 2), lwd = 2)
}

# Arrange plots
par(mfrow = c(2, 2))  # 2x2 grid

# Draw plots
plot_overlay(training_data$Temperature, training_data$Timestamp, 
             session_data$Temperature, session_data$Timestamp,
             ylabel = "Temperature", title = "Temperature: Training vs Session")

plot_overlay(training_data$Moisture, training_data$Timestamp, 
             session_data$Moisture, session_data$Timestamp,
             ylabel = "Moisture", title = "Moisture: Training vs Session")

plot_overlay(training_data$Light, training_data$Timestamp,
             session_data$Light, session_data$Timestamp,
             ylabel = "Light", title = "Light: Training vs Session")

plot_overlay(training_data$Conductivity, training_data$Timestamp,
             session_data$Conductivity, session_data$Timestamp,
             ylabel = "Conductivity", title = "Conductivity: Training vs Session")

# Reset layout
par(mfrow = c(1, 1))


```

## Correlation and Covariance Matrices

In this correlation matrix, score of 1.0 or -1.0 represents a perfect (positive or negative) self-correlation and values closer to 0 show less to no correlation. The matrix above reflects the same relationships as the covariance matrix above.

``` {r correlation-covariance,  echo=FALSE}
# Step 1: Match lengths
min_len <- min(nrow(session_data), nrow(training_data))

session_mat <- as.matrix(session_data[1:min_len, c("Temperature", "Moisture", "Light", "Conductivity")])
training_mat <- as.matrix(training_data[1:min_len, c("Temperature", "Moisture", "Light", "Conductivity")])

# Step 2: Center the matrices (subtract column means)
session_centered <- scale(session_mat, center = TRUE, scale = FALSE)
training_centered <- scale(training_mat, center = TRUE, scale = FALSE)

# Step 3: Compute cross-covariance
cross_cov <- t(session_centered) %*% training_centered / (min_len - 1)
cat("CROSS-COVARIANCE MATRIX (Session vs Training)\n")
print(cross_cov)

# Step 4: Compute cross-correlation manually
# First compute std deviations
session_sd <- apply(session_centered, 2, sd)
training_sd <- apply(training_centered, 2, sd)

# Use outer product of std deviations to normalize
cross_cor <- cross_cov / outer(session_sd, training_sd)
cat("\nCROSS-CORRELATION MATRIX (Session vs Training)\n")
print(cross_cor)

```
Plotting variables against each other.

``` {r compare-relationships,  echo=FALSE}
# Create a panel function to show both session and training data on the same plot
panel.scatter <- function(x, y, ...) {
  points(x[source == "Session"], y[source == "Session"], 
         col = "#5B9BD5", pch = 16, ...)
  points(x[source == "Training"], y[source == "Training"], 
         col = "green", pch = 17, ...)
}

# Combine data for plotting
session_combined <- session_data[, c("Temperature", "Moisture", "Light", "Conductivity")]
session_combined$source <- "Session"
training_combined <- training_data[, c("Temperature", "Moisture", "Light", "Conductivity")]
training_combined$source <- "Training"

# Combine the datasets - take an equal number of rows from each to balance the visualization
min_rows <- min(nrow(session_combined), nrow(training_combined))
combined_data <- rbind(
  session_combined[1:min_rows,],
  training_combined[1:min_rows,]
)

# Convert to factors for plotting
combined_data$source <- as.factor(combined_data$source)

# Create comparative scatter plot matrix
pairs(combined_data[, c("Temperature", "Moisture", "Light", "Conductivity")],
      col = c("#5B9BD5", "green")[combined_data$source],
      pch = c(16, 17)[combined_data$source],
      main = "Session vs Training Data Comparison")

# Add legend
par(xpd = TRUE)
legend("bottomright", legend = c("Session", "Training"),
       col = c("#5B9BD5", "green"), pch = c(16, 17))

```

Up until this point (excluding the matrecies) we have been treating the session and training data as independent data sets for comparison purposes. Instead of running similarity algorithms within each variable, both the independent (`session_data`) and the dependent (`training_data`) must be used together in order to produce valuable insights about their relationship.

## Cross Correlation Heat Map

``` {r heatmap,  echo=FALSE}
library(reshape2)
library(ggplot2)

# Set row and column names for clarity (optional but helpful)
rownames(cross_cor) <- colnames(session_mat)
colnames(cross_cor) <- colnames(training_mat)

# Melt into long format
cross_cor_long <- reshape2::melt(cross_cor)
names(cross_cor_long) <- c("Session_Variable", "Training_Variable", "CrossCorrelation")

ggplot(cross_cor_long, aes(x = Training_Variable, y = Session_Variable, fill = CrossCorrelation)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white",
                       midpoint = 0, limit = c(-1, 1),
                       name = "Cross-Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(title = "Cross-Correlation Heatmap: Session vs Training",
       x = "Training Variables", y = "Session Variables")

```

# Stage 2: Similarities and RSME

In this stage the goal is to measure the similarities and differences between our session and training data. From there, we'll ba able to classify and label the features for K means clustering in Stage 3.

### Preprocessing

``` {r similarities-preprocessing}
library(data.table)
library(dplyr)

training_dt <- as.data.table(training_data)
session_dt <- as.data.table(session_data)
# head()
```


### Cleaning the Data

To clean up the data, we'll do some indexing by setting up keys. Then to organize it, we put the fields we want to use for our analysis into their own data frames. Converting sq_ft_lot and sale_price to numeric will ensure no problems when plotting.

``` {r cleanup, echo=FALSE}

# Set keys for indexing
setkey(session_dt, `Temperature`, `Moisture`, `Light`, `Conductivity`)

# Turn important fields into their own data frames
temperature <- session_dt[["Temperature"]]
moisture <- session_dt[["Moisture"]]
light <- session_dt[["Light"]]
conductivity <- session_dt[["Conductivity"]]

temperature <- training_dt[["Temperature"]]
moisture <- training_dt[["Moisture"]]
light <- training_dt[["Light"]]
conductivity <- training_dt[["Conductivity"]]

# Convert to numeric
session_dt[, Temperature := as.numeric(as.character(Temperature))]
session_dt[, Moisture := as.numeric(as.character(Moisture))]
session_dt[, Light := as.numeric(as.character(Light))]
session_dt[, Conductivity := as.numeric(as.character(Conductivity))]

training_dt[, Temperature := as.numeric(as.character(Temperature))]
training_dt[, Moisture := as.numeric(as.character(Moisture))]
training_dt[, Light := as.numeric(as.character(Light))]
training_dt[, Conductivity := as.numeric(as.character(Conductivity))]


```

## Linear Regression Model & Errors

``` {r linear-regression, echo=FALSE}
library(ggplot2)
library(patchwork)

# Session Data Plots
p1 <- ggplot(session_dt, aes(x = Temperature, y = Moisture)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  labs(x = "Temperature", y = "Moisture")

p2 <- ggplot(session_dt, aes(x = Temperature, y = Light)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  labs(x = "Temperature", y = "Light")

p3 <- ggplot(session_dt, aes(x = Temperature, y = Conductivity)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  labs(x = "Temperature", y = "Conductivity")

# Training Data Plots
p4 <- ggplot(training_dt, aes(x = Temperature, y = Moisture)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  labs(x = "Temperature", y = "Moisture")

p5 <- ggplot(training_dt, aes(x = Temperature, y = Light)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  labs(x = "Temperature", y = "Light")

p6 <- ggplot(training_dt, aes(x = Temperature, y = Conductivity)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  labs(x = "Temperature", y = "Conductivity")

# Combine into 3 rows Ã— 2 columns
(p1 + p2 + p3 + p4 + p5 + p6) + 
  plot_layout(ncol = 2, nrow = 3)


```

This plot is nice, but can only tell us so much about the data. Running a `summary()` will give us more information.

## Session Summaries
AGT session data

``` {r summary-session, echo=FALSE}
# Install and load stargazer package if needed
# install.packages("stargazer")
library(stargazer)

# Simple (one-to-one) models
temp_model_simple <- lm(training_data$Temperature ~ session_data$Temperature)
moisture_model_simple <- lm(training_data$Moisture ~ session_data$Moisture)
light_model_simple <- lm(training_data$Light ~ session_data$Light)
conductivity_model_simple <- lm(training_data$Conductivity ~ session_data$Conductivity)

# Full (one-to-many) models
temp_model_full <- lm(training_data$Temperature ~ session_data$Temperature + session_data$Moisture + session_data$Light + session_data$Conductivity)
moisture_model_full <- lm(training_data$Moisture ~ session_data$Temperature + session_data$Moisture + session_data$Light + session_data$Conductivity)
light_model_full <- lm(training_data$Light ~ session_data$Temperature + session_data$Moisture + session_data$Light + session_data$Conductivity)
conductivity_model_full <- lm(training_data$Conductivity ~ session_data$Temperature + session_data$Moisture + session_data$Light + session_data$Conductivity)

stargazer(temp_model_simple, temp_model_full,
          title = "Temperature Model: Simple vs Full",
          type = "text")

stargazer(moisture_model_simple, temp_model_full,
          title = "Moisture Model: Simple vs Full",
          type = "text")

stargazer(light_model_simple, temp_model_full,
          title = "Light Model: Simple vs Full",
          type = "text")

stargazer(conductivity_model_simple, temp_model_full,
          title = "Conductivity Model: Simple vs Full",
          type = "text")


```

### Residual Standard Error
Residuals represent the differences between the actual and predicted values of our dependent (response) variable, sq_ft_lot. A high RSE indicates a weak model for this prediction.

### Multiple R
Multiple R, also called the correlation coefficient, measures the strength and direction of a relationship between variables. On a scale of -1 to +1, values closer to -1 or +1 represent perfect negative or positive correlation. 0 means no correlation at all.

### Multiple R2 Error
R squared tells us the proportion for variance in sq_ft_lot explained by sale_price, the predictor variable. It can be on a scale of 0 to 1. A value closer to 1 represents greater variance, while closer to 0 represents less variance. Returning a whole 0 or 1 means none or perfect variance respectively.

### Adjusted R2 Error
This error is a modified version of the above R-squared error that is able to accommodate for multiple predictors in a regression model.

# Stage 3: Modeling, Classification and Metrics

In this next section, we will use the KNN algorithm to find the best K for a label. As this is a placeholder for future use, a good question to use this for is: 

How can we classify a subjects health by having thresholds of poor, unsatisfactory, neutral, satisfactory or excellent? 

To measure this hypothetical threshold, we would use how closely or different the training data is from testing. This was explored in the previous stage, and is required to be able to have a label to predict for in KNN.

``` {r knn-modeling, echo=FALSE}
set.seed(42)  # for reproducibility

# Select relevant features
features <- c("Temperature", "Moisture", "Light", "Conductivity")

# Scale both datasets so clustering is fair
session_scaled <- scale(session_data[, features])
training_scaled <- scale(training_data[, features])

# Decide on number of clusters (we'll use 2 for simplicity, but we can analyze more)
k <- 2
session_kmeans <- kmeans(session_scaled, centers = k)
training_kmeans <- kmeans(training_scaled, centers = k)

# Add cluster labels back to original data
session_data$Cluster <- factor(session_kmeans$cluster)
training_data$Cluster <- factor(training_kmeans$cluster)

# Summary: percentage of points in each cluster
session_cluster_percent <- prop.table(table(session_data$Cluster)) * 100
training_cluster_percent <- prop.table(table(training_data$Cluster)) * 100

cat("Session Cluster Distribution (%):\n")
print(session_cluster_percent)

cat("\nTraining Cluster Distribution (%):\n")
print(training_cluster_percent)

# Optional: Show cluster centers
cat("\nSession Cluster Centers (scaled):\n")
print(session_kmeans$centers)

cat("\nTraining Cluster Centers (scaled):\n")
print(training_kmeans$centers)

```

``` {r visualize-knn,  echo=FALSE}
library(gridExtra)
library(ggplot2)

# Reduce to 2D using PCA
session_pca <- prcomp(session_scaled)
training_pca <- prcomp(training_scaled)

# Plot PCA results with clusters
session_df <- data.frame(session_pca$x[,1:2], Cluster = session_data$Cluster)
training_df <- data.frame(training_pca$x[,1:2], Cluster = training_data$Cluster)

# Plot side by side
library(gridExtra)

p1 <- ggplot(session_df, aes(x = PC1, y = PC2, color = Cluster)) +
  geom_point(size = 2) +
  theme_minimal() +
  ggtitle("Session Data Clusters")

p2 <- ggplot(training_df, aes(x = PC1, y = PC2, color = Cluster)) +
  geom_point(size = 2) +
  theme_minimal() +
  ggtitle("Training Data Clusters")

grid.arrange(p1, p2, ncol = 2)

```

In order to begin preprocessing and find the right k, we will only keep the relevant features and label the cluster. This analysis uses the session data as testing data and the generated ideal conditions as the training data.

We will use two approaches. First with the built in K nearest neighbor functions with R and then manually calculating the accuracy of each K values.

### Method 1: Built in knn() Function

``` {r knn-preprocessing}
# Select numeric features and the cluster label
features_train <- training_data[, c("Temperature", "Moisture", "Light", "Conductivity")]
labels_train <- as.factor(training_data$Cluster)

features_test <- session_data[, c("Temperature", "Moisture", "Light", "Conductivity")]
labels_test <- as.factor(session_data$Cluster)

```

Normalizing the data will help scale larger values down to a comparable size.

``` {r normalize-data}
# Normalize (scale) the features
features_train_scaled <- as.data.frame(scale(features_train))
features_test_scaled <- as.data.frame(scale(features_test))

```

For our train/test split, 

``` {r train-test-split}
train_features <- features_train_scaled
train_labels <- labels_train
```

Now we'll proceed with modeling KNN and evaluating its performance. Before we start though, this cross correlation data needs to be fitted a bit more in order to pass through the KNN model.

``` {r fit-to-knn }
# Make sure train and test sets have the same number of rows as their respective labels
min_train_rows <- min(nrow(train_features), length(train_labels))
min_test_rows <- min(nrow(test_features), length(test_labels))

# Trim all data to match
train_features <- train_features[1:min_train_rows, ]
train_labels <- train_labels[1:min_train_rows]
test_features <- test_features[1:min_test_rows, ]
test_labels <- test_labels[1:min_test_rows]

# Convert labels to factors after trimming
train_labels <- as.factor(train_labels)
test_labels <- as.factor(test_labels)
```

``` {r knn-modeling, eval=FALSE}
library(class)

# Training the KNN model
k_value <- 3  # You can experiment with different values of k
knn_model <- knn(train_features, test_features, train_labels, k = k_value)
test_features
test_labels
train_features
train_labels
# Evaluate the model
confusion_matrix <- table(Predicted = knn_model, Actual = test_labels)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)

cat("Accuracy on Session Data (Test Data):", accuracy)

```

### Method 2: Manual K Value Accuracy Computation

K Nearest Neighbor (KNN) works by computing the Euclidean distance between the test and training points. Then after selecting the proper k that is the shortest distance, it assigns those closest neighbors most common label to the test point.

Using `knn()` the `class` package automatically computes the Euclidean Distance between two points. We will adjust in the input parameters to ingest the train and test data we have already prepared.

``` {r input-config, eval=FALSE}
# Inputs for manual model
train_data <- features_train_scaled
test_data <- features_test_scaled
train_labels <- as.numeric(labels_train)
test_labels <- as.numeric(labels_test)

```

``` {r euclidean-formula, eval=FALSE}
euc_dis <- function(p1, p2) {
  sqrt(sum((p1 - p2)^2))
}

```

In this next section we're implementing the KNN Classifier manually to train the binary classifier data. At this stage, the classifier logic is being defined below.

Here is where we'll compute accuracy for the session data using manual KNN.

``` {r custom-classifier, eval=FALSE}
k_values <- seq(1, 15, 2)  # or however many k's you want

cat("Train samples:", nrow(train_data), "Label count:", length(train_labels), "\n")

accuracy_results <- c()

for (k in k_values) {
  predictions <- knn(train_data, train_labels, test_data, k)
  acc <- mean(predictions == test_labels)
  accuracy_results <- c(accuracy_results, acc)
  cat("k =", k, ", Accuracy =", round(acc * 100, 2), "%\n")
}


```

The accuracy of this model varies based in the k value used. Plotting them will provide a better idea of which to use to get the best accuracy.

``` {r plot-k-accuracy, eval=FALSE}
accuracy_data <- data.frame(
  k_values = k_values,
  accuracy = accuracy_results,
  dataset = "Session/Training"
)

ggplot(accuracy_data, aes(x = k_values, y = accuracy)) +
  geom_line(color = "darkgreen") +
  geom_point(size = 3, color = "darkgreen") +
  labs(title = "Manual KNN Accuracy vs. k (Agrotech Session Data)",
       x = "k (Number of Neighbors)", y = "Accuracy") +
  theme_minimal()

```

# Stage 4: Forecasting

# Stage 5: Discussion
